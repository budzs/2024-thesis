{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "\n",
    "# Ensure compatibility with different TensorFlow versions\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = 'C:\\\\Users\\\\MindRove_BZs\\\\Diploma\\\\processed_data_notch_norm\\\\'\n",
    "class_dir = 'C:\\\\Users\\\\MindRove_BZs\\\\Diploma\\\\classes\\\\'\n",
    "\n",
    "def load_data(patient_ids):\n",
    "    emg_data = {}\n",
    "    eeg_data = {}\n",
    "    for i in patient_ids:\n",
    "        with open(os.path.join(data_dir, f'normalized_filtered_emg_data_patient_{i}.pkl'), 'rb') as f:\n",
    "            patient_emg_data = pickle.load(f)\n",
    "            for key in patient_emg_data:\n",
    "                if isinstance(key, float) and key.is_integer():\n",
    "                    key = int(key) \n",
    "                modified_key = f\"patient_{i}_{key}\"\n",
    "                emg_data[modified_key] = patient_emg_data[key]\n",
    "\n",
    "        with open(os.path.join(data_dir, f'normalized_filtered_eeg_data_patient_{i}.pkl'), 'rb') as f:\n",
    "            patient_eeg_data = pickle.load(f)\n",
    "            for key in patient_eeg_data:\n",
    "                if isinstance(key, float) and key.is_integer():\n",
    "                    key = int(key)  \n",
    "                modified_key = f\"patient_{i}_{key}\"\n",
    "                eeg_data[modified_key] = patient_eeg_data[key]\n",
    "        print(f\"Size of eeg data after patient {i}: {len(eeg_data)}\")\n",
    "    \n",
    "    return emg_data, eeg_data\n",
    "\n",
    "def load_classes():\n",
    "    class_files = [f for f in os.listdir(class_dir) if f.endswith('.pickle')]\n",
    "    all_classes = {}\n",
    "    for file in class_files:\n",
    "        with open(os.path.join(class_dir, file), 'rb') as f:\n",
    "            classes_data = pickle.load(f)\n",
    "            identifier = file.split('_')[-1].replace('.pickle', '')\n",
    "            for key in classes_data:\n",
    "                modified_key = f\"patient_{identifier}_{key}\"\n",
    "                all_classes[modified_key] = classes_data[key]\n",
    "    return all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(emg_data, eeg_data):\n",
    "    channels_to_keep = np.r_[0:6, 7:8]\n",
    "    filtered_emg_data = {}\n",
    "    filtered_eeg_data = {}\n",
    "    for frame in emg_data:\n",
    "        if emg_data[frame].shape[1] >= 30 and eeg_data[frame].shape[1] >= 30:\n",
    "            min_length = 30\n",
    "            filtered_emg_data[frame] = emg_data[frame][channels_to_keep][:, :min_length]\n",
    "            filtered_eeg_data[frame] = eeg_data[frame][channels_to_keep][:, :min_length]\n",
    "    return filtered_emg_data, filtered_eeg_data\n",
    "\n",
    "\n",
    "def prepare_dataset(emg_data, eeg_data, all_classes):\n",
    "    filtered_emg_data, filtered_eeg_data = filter_data(emg_data, eeg_data)\n",
    "    X = []\n",
    "    y = []\n",
    "    for frame in all_classes.keys():\n",
    "        \n",
    "        if frame in filtered_emg_data and frame in filtered_eeg_data:\n",
    "            combined_data = np.concatenate((filtered_emg_data[frame], filtered_eeg_data[frame]), axis=0)\n",
    "            X.append(combined_data)\n",
    "            y.append(all_classes[frame])\n",
    "    X = np.array(X)\n",
    "    y = to_categorical(np.array(y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "def build_lstm_model(input_shape=(14, 30, 1), num_classes=8, learning_rate=0.001):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1]))(inputs)\n",
    "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x) \n",
    "    x = tf.keras.layers.BatchNormalization()(x) \n",
    "    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)  \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LSTM(64)(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x) \n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss, val_loss, acc, val_acc):\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, 'b-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, acc, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ensure_one_hot_encoding(y):\n",
    "    if len(y.shape) == 1:\n",
    "        n_classes = np.max(y) + 1\n",
    "        y = to_categorical(y, num_classes=n_classes)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10,  \n",
    "    restore_best_weights=True \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(14, 30)))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(BatchNormalization())  \n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dropout(0.2))  \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "lr_schedule = schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    patient_ids = range(5, 12)\n",
    "    emg_data, eeg_data = load_data(patient_ids)\n",
    "    print(f\"Number of frames in EMG data: {len(emg_data)}\")\n",
    "    print(f\"Number of frames in EEG data: {len(eeg_data)}\")\n",
    "    \n",
    "    all_classes = load_classes()\n",
    "    print(f\"Number of frames in all classes: {len(all_classes)}\")    \n",
    "\n",
    "    X, y = prepare_dataset(emg_data, eeg_data, all_classes)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "    X_train = X_train.reshape((-1, 14, 30, 1))\n",
    "    X_test = X_test.reshape((-1, 14, 30, 1))\n",
    "\n",
    "    y_train = ensure_one_hot_encoding(y_train)\n",
    "    y_test = ensure_one_hot_encoding(y_test)\n",
    "\n",
    "    print(f\"y_train shape (after check): {y_train.shape}\")\n",
    "    print(f\"y_test shape (after check): {y_test.shape}\")\n",
    "        \n",
    "    #lstm_model = build_lstm_model(input_shape=(14, 30, 1), num_classes=8)\n",
    "    #lstm_model.summary()\n",
    "    history = model.fit(X_train, y_train, epochs=1000, batch_size=800, validation_data=(X_test, y_test),class_weight=class_weights_dict )\n",
    "                             \n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    all_classes = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    print(f\"Length of all_classes: {len(all_classes)}\")\n",
    "    print(f\"Content of all_classes: {all_classes}\")\n",
    "    all_classes_str = [str(c) for c in all_classes]\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=all_classes_str))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plot_confusion_matrix(cm, classes=all_classes_str)\n",
    "    plt.show()\n",
    "\n",
    "    evaluation_results = model.evaluate(X_test, y_test, verbose=2)\n",
    "    test_loss, test_accuracy = evaluation_results[:2]\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    model.save('C:\\\\Users\\\\MindRove_BZs\\\\Diploma\\\\models\\\\model_results.h5')\n",
    "    plot(history.history['loss'], history.history['val_loss'], history.history['accuracy'], history.history['val_accuracy'])\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
